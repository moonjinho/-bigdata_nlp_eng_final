{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyP7U1wa2kCohtRtOGWDLH1i"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzuqszV25FcG"
      },
      "source": [
        "필요한 라이브러리 import 및 데이터 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en_H2YtPw_HS",
        "outputId": "31c5c572-6032-472a-a09e-18c3563dbd4e"
      },
      "source": [
        "import json\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import sys\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tag import pos_tag\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk import Text\r\n",
        "import collections\r\n",
        "from keras.layers.core import Dense, SpatialDropout1D \r\n",
        "from keras.layers.convolutional import Conv1D \r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from keras.layers.pooling import GlobalMaxPooling1D\r\n",
        "from keras.layers import LSTM, Dropout, MaxPooling1D\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.preprocessing.sequence import pad_sequences \r\n",
        "from keras.utils import np_utils \r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "stops = set(stopwords.words('english'))\r\n",
        "stemmer = nltk.stem.SnowballStemmer('english')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZEOCccV5L4b"
      },
      "source": [
        "데이터 진행 과정 확인을 위한 Progress Bar Util"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0HDwfjG5QdE"
      },
      "source": [
        "def printProgress (iteration, total, prefix = '', suffix = '', decimals = 1, barLength = 100):\r\n",
        "    formatStr = \"{0:.\" + str(decimals) + \"f}\"\r\n",
        "    percent = formatStr.format(100 * (iteration / float(total)))\r\n",
        "    filledLength = int(round(barLength * iteration / float(total)))\r\n",
        "    bar = '#' * filledLength + '-' * (barLength - filledLength)\r\n",
        "    sys.stdout.write('\\r%s |%s| %s%s %s' % (prefix, bar, percent, '%', suffix)),\r\n",
        "    if iteration == total:\r\n",
        "        sys.stdout.write('\\n')\r\n",
        "    sys.stdout.flush()"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49IAVxCp5UD3"
      },
      "source": [
        "json 데이터 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m86GuaxPxG8O"
      },
      "source": [
        "with open('friends_train.json') as json_file:\r\n",
        "    json_train = json.load(json_file)\r\n",
        "with open('friends_test.json') as json_file:\r\n",
        "    json_test = json.load(json_file)\r\n",
        "with open('friends_dev.json') as json_file:\r\n",
        "    json_dev = json.load(json_file)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPNhfMpH5d9D"
      },
      "source": [
        "영어 데이터 전처리 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMukqKxNxP2l"
      },
      "source": [
        "def cleaning(str):\r\n",
        "    replaceAll = str\r\n",
        "    only_english = re.sub('[^a-zA-Z]', ' ', replaceAll)\r\n",
        "    no_capitals = only_english.lower().split()\r\n",
        "    no_stops = [word for word in no_capitals if not word in stops]\r\n",
        "    stemmer_words = [stemmer.stem(word) for word in no_stops]\r\n",
        "    return ' '.join(stemmer_words)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G0C9X5t53pN"
      },
      "source": [
        "트레이닝용 데이터 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBI4YAXAxSr6"
      },
      "source": [
        "i = 0\r\n",
        "train_data=[]\r\n",
        "for rows in json_train:\r\n",
        "    for row in rows:\r\n",
        "        train_data.append([cleaning(row['utterance']), row['emotion']])\r\n",
        "for rows in json_test:\r\n",
        "    for row in rows:\r\n",
        "        train_data.append([cleaning(row['utterance']), row['emotion']])\r\n",
        "for rows in json_dev:\r\n",
        "    for row in rows:\r\n",
        "        train_data.append([cleaning(row['utterance']), row['emotion']])"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "redqaFT26K_n"
      },
      "source": [
        "품사 태깅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl0MiK8_xYHx",
        "outputId": "af88ae37-2c95-42a0-b5a8-06c35ef4f307"
      },
      "source": [
        "cnt = 0\r\n",
        "tagged = []\r\n",
        "counter = collections.Counter()\r\n",
        "for d in train_data:\r\n",
        "    cnt = cnt + 1\r\n",
        "    printProgress(cnt, len(train_data), 'Progress:', 'Complete', 1, 100)\r\n",
        "    words = pos_tag(word_tokenize(d[0]))\r\n",
        "    for t in words:\r\n",
        "        word = \"/\".join(t)\r\n",
        "        tagged.append(word)\r\n",
        "        counter[word] += 1"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: |####################################################################################################| 100.0% Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P74wgezS6R4p"
      },
      "source": [
        "카운팅 끝난 후, 임베딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEHaM-HcxaR5"
      },
      "source": [
        "VOCAB_SIZE = 5000\r\n",
        "word2index = collections.defaultdict(int)\r\n",
        "for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\r\n",
        "    word2index[word[0]] = wid + 1\r\n",
        "vocab_sz = len(word2index) + 1\r\n",
        "index2word = {v:k for k, v in word2index.items()}"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rry5Fz06dT_"
      },
      "source": [
        "감정을 번호로, 번호를 감정으로 변환해주는 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFx9GlU9xcpZ"
      },
      "source": [
        "def labeltoint(str):\r\n",
        "    return {'non-neutral': 0,\r\n",
        "             'neutral': 1, \r\n",
        "             'joy': 2,\r\n",
        "             'sadness': 3,\r\n",
        "             'fear': 4,\r\n",
        "             'anger': 5,\r\n",
        "             'surprise': 6,\r\n",
        "             'disgust': 7}[str]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcHivnV06kHu"
      },
      "source": [
        "def inttolabel(idx):\r\n",
        "    return {0:'non-neutral',\r\n",
        "             1:'neutral', \r\n",
        "             2:'joy',\r\n",
        "             3:'sadness',\r\n",
        "             4:'fear',\r\n",
        "             5:'anger',\r\n",
        "             6:'surprise',\r\n",
        "             7:'disgust'}[idx]"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP8CrEz66pzP"
      },
      "source": [
        "트레이닝 데이터의 emotion value를 모두 숫자로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5S5WFd1xfKx",
        "outputId": "469813b3-93fb-446b-c232-3743b56f305f"
      },
      "source": [
        "xs, ys = [], []\r\n",
        "cnt = 0\r\n",
        "maxlen = 0\r\n",
        "for d in train_data:\r\n",
        "    cnt = cnt + 1\r\n",
        "    ys.append(labeltoint(d[1]))\r\n",
        "    printProgress(cnt, len(train_data), 'Progress:', 'Complete', 1, 100)\r\n",
        "    ang = pos_tag(word_tokenize(d[0]))\r\n",
        "    words=[]\r\n",
        "    for t in ang:\r\n",
        "        words.append(\"/\".join(t))\r\n",
        "    if len(words) > maxlen: \r\n",
        "        maxlen = len(words)\r\n",
        "    wids = [word2index[word] for word in words]\r\n",
        "    xs.append(wids)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: |####################################################################################################| 100.0% Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4Kg1qfY62tQ"
      },
      "source": [
        "X,Y 에 최종적인 train data와 train label 투입 후, 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXiFtmcV96uZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21cc5cbf-c971-4563-fdbc-b212d1346a09"
      },
      "source": [
        "X = pad_sequences(xs, maxlen=maxlen) \r\n",
        "Y = np_utils.to_categorical(ys)\r\n",
        "\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=50)\r\n",
        "\r\n",
        "model = Sequential() \r\n",
        "model.add(Embedding(vocab_sz, 100, input_length=maxlen))\r\n",
        "model.add(LSTM(100, activation='tanh')) \r\n",
        "model.add(Dense(8, activation=\"softmax\"))\r\n",
        "\r\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\r\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_accuracy', mode='max', verbose=1, save_best_only=True)\r\n",
        "\r\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) \r\n",
        "history = model.fit(x_train, y_train, epochs=20, batch_size=64, callbacks=[es, mc], validation_data=(x_test, y_test))\r\n",
        "# history = model.fit(x_train, y_train, epochs=20, batch_size=64, validation_data=(x_test, y_test))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "159/159 [==============================] - 12s 66ms/step - loss: 1.6949 - accuracy: 0.4343 - val_loss: 1.5506 - val_accuracy: 0.4629\n",
            "\n",
            "Epoch 00001: val_accuracy improved from -inf to 0.46288, saving model to best_model.h5\n",
            "Epoch 2/20\n",
            "159/159 [==============================] - 10s 63ms/step - loss: 1.4689 - accuracy: 0.4779 - val_loss: 1.4888 - val_accuracy: 0.4629\n",
            "\n",
            "Epoch 00002: val_accuracy did not improve from 0.46288\n",
            "Epoch 3/20\n",
            "159/159 [==============================] - 10s 64ms/step - loss: 1.2571 - accuracy: 0.5525 - val_loss: 1.5311 - val_accuracy: 0.4663\n",
            "\n",
            "Epoch 00003: val_accuracy improved from 0.46288 to 0.46633, saving model to best_model.h5\n",
            "Epoch 4/20\n",
            "159/159 [==============================] - 10s 63ms/step - loss: 1.0705 - accuracy: 0.6241 - val_loss: 1.6328 - val_accuracy: 0.4553\n",
            "\n",
            "Epoch 00004: val_accuracy did not improve from 0.46633\n",
            "Epoch 5/20\n",
            "159/159 [==============================] - 10s 63ms/step - loss: 0.9656 - accuracy: 0.6615 - val_loss: 1.7541 - val_accuracy: 0.4387\n",
            "\n",
            "Epoch 00005: val_accuracy did not improve from 0.46633\n",
            "Epoch 6/20\n",
            "159/159 [==============================] - 10s 63ms/step - loss: 0.8590 - accuracy: 0.6976 - val_loss: 1.8785 - val_accuracy: 0.4376\n",
            "\n",
            "Epoch 00006: val_accuracy did not improve from 0.46633\n",
            "Epoch 00006: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bchxw5DtQaLi"
      },
      "source": [
        "가장 좋은 모델을 기준으로 테스트 데이터 정확도 측정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQJQWg98QZ2o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60833f5d-876b-47ae-8011-5adf6c7cb994"
      },
      "source": [
        "loaded_model = load_model('best_model.h5')\r\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(x_test, y_test)[1]))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "136/136 [==============================] - 2s 11ms/step - loss: 1.5311 - accuracy: 0.4663\n",
            "\n",
            " 테스트 정확도: 0.4663\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9YX6IIWRzu8"
      },
      "source": [
        "새로운 문장이 들어왔을 때 감정을 분석하는 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBjs46HJyts3"
      },
      "source": [
        "def sentiment_predict(text): \r\n",
        "    aa = pos_tag(word_tokenize(text))\r\n",
        "    pp = []\r\n",
        "    for t in aa:\r\n",
        "        pp.append(\"/\".join(t))\r\n",
        "    wids = [word2index[word] for word in pp]\r\n",
        "    x_predict = pad_sequences([wids], maxlen=maxlen) \r\n",
        "    y_predict = loaded_model.predict(x_predict)\r\n",
        "    c = 0\r\n",
        "    cnt = 0\r\n",
        "    for y in y_predict[0]:\r\n",
        "        if c < y:\r\n",
        "            c = y\r\n",
        "            ans = cnt\r\n",
        "        cnt += 1\r\n",
        "    ans = inttolabel(ans)\r\n",
        "    return ans;"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wubxfjMRto4"
      },
      "source": [
        "Kaggle Sample 데이터를 통한 결과 분류 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOrwV77i0fGw",
        "outputId": "36ea6d44-54f1-4674-d2b5-bef85f2c9ea7"
      },
      "source": [
        "predict_data = pd.read_csv('data.csv')\r\n",
        "total_length = len(predict_data)\r\n",
        "\r\n",
        "cols = ['Id', 'Predicted']\r\n",
        "lst = []\r\n",
        "for index, row in predict_data.iterrows():\r\n",
        "    printProgress(row['id'], total_length, 'Progress:', 'Complete', 1, 100)\r\n",
        "    lst.append([row['id'], sentiment_predict(row['utterance'])])\r\n",
        "\r\n",
        "result_df = pd.DataFrame(lst, columns=cols)\r\n",
        "print(result_df)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: |####################################################################################################| 99.9% Complete        Id    Predicted\n",
            "0        0      neutral\n",
            "1        1      neutral\n",
            "2        2      neutral\n",
            "3        3      neutral\n",
            "4        4      neutral\n",
            "...    ...          ...\n",
            "1618  1618      neutral\n",
            "1619  1619      neutral\n",
            "1620  1620      neutral\n",
            "1621  1621      neutral\n",
            "1622  1622  non-neutral\n",
            "\n",
            "[1623 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2s7tjNjRv_U"
      },
      "source": [
        "분석된 결과를 CSV 파일로 export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIxrY3RMy8n-"
      },
      "source": [
        "result_df.to_csv('sample.csv', sep=',', index=False)"
      ],
      "execution_count": 17,
      "outputs": []
    }
  ]
}