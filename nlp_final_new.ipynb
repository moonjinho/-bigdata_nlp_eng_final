{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNdbzXTnkqepzR3r38gZEr1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puthvbFN8rOv"
      },
      "source": [
        "필요한 라이브러리 import 및 데이터 가져오기\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "w2eios8fjDe_"
      },
      "source": [
        "!pip install konlpy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SKzOf2w6lRbB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "940824cb-ba2d-4876-9fe0-e3b9edaf09db"
      },
      "source": [
        "import pandas as pd\r\n",
        "import numpy as np\r\n",
        "%matplotlib inline\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import re\r\n",
        "import sys\r\n",
        "import json\r\n",
        "import nltk\r\n",
        "nltk.download('stopwords')\r\n",
        "\r\n",
        "from konlpy.tag import Okt\r\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\r\n",
        "\r\n",
        "from nltk.corpus import stopwords \r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.stem.lancaster import LancasterStemmer\r\n",
        "from nltk.stem.snowball import SnowballStemmer\r\n",
        "from nltk.stem import WordNetLemmatizer\r\n",
        "\r\n",
        "\r\n",
        "dev_file = open('friends_test.json', encoding=\"utf-8\")\r\n",
        "dev_data = json.load(dev_file)\r\n",
        "train_file = open('friends_train.json', encoding=\"utf-8\")\r\n",
        "train_data = json.load(train_file)\r\n",
        "test_file = open('friends_dev.json', encoding=\"utf-8\")\r\n",
        "test_data = json.load(test_file)\r\n",
        "\r\n",
        "df_dev = pd.DataFrame(columns=['annotation', 'emotion', 'speaker', 'utterance'])\r\n",
        "df_train = pd.DataFrame(columns=['annotation', 'emotion', 'speaker', 'utterance'])\r\n",
        "df_test = pd.DataFrame(columns=['annotation', 'emotion', 'speaker', 'utterance'])\r\n",
        "\r\n",
        "for i in range(len(dev_data)):\r\n",
        "    df_dev = pd.concat([df_dev, pd.DataFrame(dev_data[i])])\r\n",
        "\r\n",
        "for i in range(len(train_data)):\r\n",
        "    df_train = pd.concat([df_train, pd.DataFrame(train_data[i])])\r\n",
        "\r\n",
        "for i in range(len(test_data)):\r\n",
        "    df_test = pd.concat([df_test, pd.DataFrame(test_data[i])])\r\n",
        "\r\n",
        "df_dev = df_dev.reset_index(drop=True)\r\n",
        "df_train = df_train.reset_index(drop=True)\r\n",
        "df_test = df_test.reset_index(drop=True)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MZpkos6ELSZn"
      },
      "source": [
        "데이터 진행 과정 확인을 위한 Progress Bar Util"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHIRT1fqLJdg"
      },
      "source": [
        "def printProgress (iteration, total, prefix = '', suffix = '', decimals = 1, barLength = 100):\r\n",
        "    formatStr = \"{0:.\" + str(decimals) + \"f}\"\r\n",
        "    percent = formatStr.format(100 * (iteration / float(total)))\r\n",
        "    filledLength = int(round(barLength * iteration / float(total)))\r\n",
        "    bar = '#' * filledLength + '-' * (barLength - filledLength)\r\n",
        "    sys.stdout.write('\\r%s |%s| %s%s %s' % (prefix, bar, percent, '%', suffix)),\r\n",
        "    if iteration == total:\r\n",
        "        sys.stdout.write('\\n')\r\n",
        "    sys.stdout.flush()"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qT8vbKpGoYzj"
      },
      "source": [
        "영어 데이터 전처리 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C20rG1X3oXwi"
      },
      "source": [
        "lancaster_stemmer = LancasterStemmer()\r\n",
        "stemmer = SnowballStemmer('english') \r\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\r\n",
        "\r\n",
        "stops = set(stopwords.words('english'))\r\n",
        "\r\n",
        "df_dev['words'] = ''\r\n",
        "df_train['words'] = ''\r\n",
        "df_test['words'] = ''\r\n",
        "\r\n",
        "def comment_to_words(data):\r\n",
        "    # 1. 영어가 아닌 문자는 공백으로 변환\r\n",
        "    data = re.sub('[^a-zA-Z]', ' ', data)\r\n",
        "    \r\n",
        "    # 2. 소문자로 변환\r\n",
        "    lowerdata = data.lower()\r\n",
        "    \r\n",
        "    # 3. 문자열로 변환\r\n",
        "    words = lowerdata.split()\r\n",
        "    \r\n",
        "    # 4. 불용어 제거\r\n",
        "    words = [w for w in words if not w in stops]\r\n",
        "    \r\n",
        "    # 5. 어간추출\r\n",
        "    stemming_words = [stemmer.stem(w) for w in words]\r\n",
        "    \r\n",
        "    # 7. 공백으로 구분된 문자열로 결합하여 결과를 반환\r\n",
        "    words = ' '.join(stemming_words)\r\n",
        "    return words"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d9FjlqvS9avC"
      },
      "source": [
        "훈련 & 개발 & 테스트 데이터 전처리"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "iE5wOPk49eEj"
      },
      "source": [
        "# 개발 데이터셋 전처리\r\n",
        "for i in range(0, len(df_dev)):\r\n",
        "    df_dev.loc[i, 'words'] = comment_to_words(df_dev.loc[i, 'utterance'])\r\n",
        "\r\n",
        "# 훈련 데이터셋 전처리\r\n",
        "for i in range(0, len(df_train)):\r\n",
        "    df_train.loc[i, 'words'] = comment_to_words(df_train.loc[i, 'utterance'])    \r\n",
        "\r\n",
        "# 테스트 데이터셋 전처리\r\n",
        "for i in range(0, len(df_test)):\r\n",
        "    df_test.loc[i, 'words'] = comment_to_words(df_test.loc[i, 'utterance'])"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7okSxeTxpmaa"
      },
      "source": [
        "y_info = [['neutral', 'neutral'],\r\n",
        "          ['surprise', 'surprise'],\r\n",
        "          ['non-neutral', 'non-neutral'],\r\n",
        "          ['joy', 'joy'],\r\n",
        "          ['sadness', 'sadness'],\r\n",
        "          ['anger', 'anger'],\r\n",
        "          ['disgust', 'disgust']]\r\n",
        "\r\n",
        "df_y = pd.DataFrame(y_info, columns=['emotion', 'Y'])\r\n",
        "\r\n",
        "df_dev = pd.merge(df_dev, df_y, on=['emotion'])\r\n",
        "df_train = pd.merge(df_train, df_y, on=['emotion'])\r\n",
        "df_test = pd.merge(df_test, df_y, on=['emotion'])"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nKh6A0_6q0kq"
      },
      "source": [
        "Tokenizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "75WwOz7jV4O1",
        "outputId": "0e24138b-aaba-4376-bb42-aa73a47ac045"
      },
      "source": [
        "from tensorflow.keras import models\r\n",
        "from tensorflow.keras import layers\r\n",
        "from tensorflow.keras import optimizers\r\n",
        "from tensorflow.keras import losses\r\n",
        "from tensorflow.keras import metrics\r\n",
        "\r\n",
        "from keras.preprocessing.text import Tokenizer\r\n",
        "from keras.preprocessing import sequence\r\n",
        "from keras import backend as K\r\n",
        "\r\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\r\n",
        "from sklearn.linear_model.logistic import LogisticRegression\r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from sklearn.model_selection  import GridSearchCV\r\n",
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\r\n",
        "from sklearn.pipeline import Pipeline"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.linear_model.logistic module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.linear_model. Anything that cannot be imported from sklearn.linear_model is now part of the private API.\n",
            "  warnings.warn(message, FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXFFfUFaq5a6",
        "outputId": "742155c9-06e1-470b-fe77-f66d892f0e50",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# The maximum number of words to be used. (most frequent)\r\n",
        "MAX_NB_WORDS = 50000\r\n",
        "\r\n",
        "# Max number of words in each complaint.\r\n",
        "MAX_SEQUENCE_LENGTH = 250\r\n",
        "\r\n",
        "# This is fixed.\r\n",
        "EMBEDDING_DIM = 100\r\n",
        "\r\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', lower=True)\r\n",
        "tokenizer.fit_on_texts(df_dev['words'].values)\r\n",
        "word_index = tokenizer.word_index\r\n",
        "\r\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1891 unique tokens.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3fg7ApQ6rRDw"
      },
      "source": [
        "패딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmUXZd4Iq_1S",
        "outputId": "3d17a536-5cf5-467a-c9a5-87533729917d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X = tokenizer.texts_to_sequences(df_train['words'].values)\r\n",
        "X = sequence.pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\r\n",
        "print('Shape of data tensor:', X.shape)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of data tensor: (10376, 250)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vj3k_JfUrWZK",
        "outputId": "f2a55c8d-03a3-48bc-9442-20dcc691a3a3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "Y = pd.get_dummies(df_train['Y']).values\r\n",
        "print('Shape of label tensor:', Y.shape)\r\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape of label tensor: (10376, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DkKglNG8rbE6",
        "outputId": "236a2d9d-c058-436a-fc2d-f1741357123d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.2, random_state = 42)\r\n",
        "\r\n",
        "print(X_train.shape,Y_train.shape)\r\n",
        "print(X_test.shape,Y_test.shape)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(8300, 250) (8300, 7)\n",
            "(2076, 250) (2076, 7)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jsbZuvct5T2"
      },
      "source": [
        "학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aqE9uTqrjNa",
        "outputId": "6b7c4402-909c-4c42-e95a-4a88ddf41686",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "EMBEDDING_DIM = 250\r\n",
        "MAX_NB_WORDS = len(X_train)\r\n",
        "\r\n",
        "import keras\r\n",
        "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense, SpatialDropout1D, LSTM\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "\r\n",
        "model = Sequential()\r\n",
        "#model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X_train.shape[0]))\r\n",
        "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM))\r\n",
        "model.add(SpatialDropout1D(0.2))\r\n",
        "model.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\r\n",
        "model.add(Dense(7, activation='sigmoid'))\r\n",
        "\r\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\r\n",
        "mc = ModelCheckpoint('best_model.h5', monitor='val_acc', mode='max', verbose=1, save_best_only=True)\r\n",
        "\r\n",
        "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['acc'])\r\n",
        "\r\n",
        "history = model.fit(X_train, \r\n",
        "                    Y_train, \r\n",
        "                    epochs=15, \r\n",
        "                    batch_size=60,\r\n",
        "                    validation_split=0.2,\r\n",
        "                    callbacks=[es, mc])"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "111/111 [==============================] - 116s 1s/step - loss: 0.3805 - acc: 0.4223 - val_loss: 0.3320 - val_acc: 0.4596\n",
            "\n",
            "Epoch 00001: val_acc improved from -inf to 0.45964, saving model to best_model.h5\n",
            "Epoch 2/15\n",
            "111/111 [==============================] - 113s 1s/step - loss: 0.3325 - acc: 0.4525 - val_loss: 0.3181 - val_acc: 0.4994\n",
            "\n",
            "Epoch 00002: val_acc improved from 0.45964 to 0.49940, saving model to best_model.h5\n",
            "Epoch 3/15\n",
            "111/111 [==============================] - 113s 1s/step - loss: 0.3124 - acc: 0.4920 - val_loss: 0.3185 - val_acc: 0.4747\n",
            "\n",
            "Epoch 00003: val_acc did not improve from 0.49940\n",
            "Epoch 4/15\n",
            "111/111 [==============================] - 114s 1s/step - loss: 0.3016 - acc: 0.5141 - val_loss: 0.3150 - val_acc: 0.4813\n",
            "\n",
            "Epoch 00004: val_acc did not improve from 0.49940\n",
            "Epoch 5/15\n",
            "111/111 [==============================] - 113s 1s/step - loss: 0.2880 - acc: 0.5465 - val_loss: 0.3151 - val_acc: 0.4849\n",
            "\n",
            "Epoch 00005: val_acc did not improve from 0.49940\n",
            "Epoch 6/15\n",
            "111/111 [==============================] - 114s 1s/step - loss: 0.2824 - acc: 0.5495 - val_loss: 0.3179 - val_acc: 0.4873\n",
            "\n",
            "Epoch 00006: val_acc did not improve from 0.49940\n",
            "Epoch 7/15\n",
            "111/111 [==============================] - 114s 1s/step - loss: 0.2728 - acc: 0.5672 - val_loss: 0.3185 - val_acc: 0.4898\n",
            "\n",
            "Epoch 00007: val_acc did not improve from 0.49940\n",
            "Epoch 8/15\n",
            "111/111 [==============================] - 114s 1s/step - loss: 0.2670 - acc: 0.5805 - val_loss: 0.3231 - val_acc: 0.4801\n",
            "\n",
            "Epoch 00008: val_acc did not improve from 0.49940\n",
            "Epoch 00008: early stopping\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FY2iUzBnsLa3"
      },
      "source": [
        "가장 좋은 모델을 기준으로 테스트 데이터 정확도 측정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RF0AcDplr_ii",
        "outputId": "4b60d854-fe04-48b9-991d-81a7874fffff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "loaded_model = load_model('best_model.h5')\r\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (loaded_model.evaluate(X_test, Y_test)[1]))"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "65/65 [==============================] - 7s 99ms/step - loss: 0.3229 - acc: 0.4812\n",
            "\n",
            " 테스트 정확도: 0.4812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E-S0HcSpP_6U"
      },
      "source": [
        "Kaggle Sample 데이터를 통한 결과 분류 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XN2sYwpH1I3i",
        "outputId": "82cbebe5-e099-4616-9da2-b756703cd41b"
      },
      "source": [
        "data = pd.read_csv(\"data.csv\", sep=\",\")\r\n",
        "data['words'] = ''\r\n",
        "\r\n",
        "print(data[:5])\r\n",
        "\r\n",
        "for i in range(0, len(data)):\r\n",
        "    data.loc[i, 'words'] = comment_to_words(data.loc[i, 'utterance']) \r\n",
        "\r\n",
        "print(data[:5])\r\n",
        "\r\n",
        "X_last = tokenizer.texts_to_sequences(data['words'].values)\r\n",
        "X_last = sequence.pad_sequences(X_last, maxlen=MAX_SEQUENCE_LENGTH)\r\n",
        "print('Shape of data tensor:', X_last.shape)\r\n",
        "\r\n",
        "predicted = loaded_model.predict(X_last)\r\n",
        "\r\n",
        "print(predict[:5])\r\n",
        "\r\n",
        "cols = ['Id', 'Predicted']\r\n",
        "result_df = pd.DataFrame([], columns=cols)\r\n",
        "\r\n",
        "id_list = []\r\n",
        "result = []\r\n",
        "result2 = []\r\n",
        "\r\n",
        "for predict in predicted:\r\n",
        "    result.append([np.argmax(predict), y_info[np.argmax(predict)][0]])\r\n",
        "    result2.append( y_info[np.argmax(predict)][0])\r\n",
        "\r\n",
        "for i in range(0, len(data)):\r\n",
        "    id_list.append(i)\r\n",
        "\r\n",
        "result_df['Id'] = id_list\r\n",
        "result_df['Predicted'] = result2"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   id  i_dialog  ...                                          utterance words\n",
            "0   0         0  ...                      Alright, whadyou do with him?      \n",
            "1   1         0  ...                                  Oh! You're awake!      \n",
            "2   2         0  ...  Then you gotta come clean with Ma! This is not...      \n",
            "3   3         0  ...                                  Yeah, but this is      \n",
            "4   4         0  ...          I don't wanna hear it! Now go to my room!      \n",
            "\n",
            "[5 rows x 6 columns]\n",
            "   id  ...                   words\n",
            "0   0  ...         alright whadyou\n",
            "1   1  ...                 oh awak\n",
            "2   2  ...  gotta come clean right\n",
            "3   3  ...                    yeah\n",
            "4   4  ...      wanna hear go room\n",
            "\n",
            "[5 rows x 6 columns]\n",
            "Shape of data tensor: (1623, 250)\n",
            "[0.03803337 0.0232929  0.20965481 0.50287366 0.11513114]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ8qh06nQETu"
      },
      "source": [
        "분석된 결과를 CSV 파일로 export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtI4Y_wa9ZdU"
      },
      "source": [
        "result_df.to_csv('sample.csv', sep=',', index=False)"
      ],
      "execution_count": 55,
      "outputs": []
    }
  ]
}