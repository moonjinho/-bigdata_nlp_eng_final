{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOp4EXI1HiMXptQvDumosL0"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzuqszV25FcG"
      },
      "source": [
        "필요한 라이브러리 import 및 데이터 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "en_H2YtPw_HS",
        "outputId": "f417f2a8-a232-4a8a-988a-afe854504756"
      },
      "source": [
        "import json\r\n",
        "import re\r\n",
        "import nltk\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import pandas as pd\r\n",
        "import sys\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tag import pos_tag\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk import Text\r\n",
        "import collections\r\n",
        "from keras.layers.core import Dense, SpatialDropout1D \r\n",
        "from keras.layers.convolutional import Conv1D \r\n",
        "from keras.layers.embeddings import Embedding\r\n",
        "from keras.layers.pooling import GlobalMaxPooling1D\r\n",
        "from keras.layers import LSTM, Dropout, MaxPooling1D\r\n",
        "from keras.models import Sequential\r\n",
        "from keras.preprocessing.sequence import pad_sequences \r\n",
        "from keras.utils import np_utils \r\n",
        "from sklearn.model_selection import train_test_split\r\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\r\n",
        "from tensorflow.keras.models import load_model\r\n",
        "\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')\r\n",
        "nltk.download('averaged_perceptron_tagger')\r\n",
        "stops = set(stopwords.words('english'))\r\n",
        "stemmer = nltk.stem.SnowballStemmer('english')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZEOCccV5L4b"
      },
      "source": [
        "데이터 진행 과정 확인을 위한 Progress Bar Util"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0HDwfjG5QdE"
      },
      "source": [
        "def printProgress (iteration, total, prefix = '', suffix = '', decimals = 1, barLength = 100):\r\n",
        "    formatStr = \"{0:.\" + str(decimals) + \"f}\"\r\n",
        "    percent = formatStr.format(100 * (iteration / float(total)))\r\n",
        "    filledLength = int(round(barLength * iteration / float(total)))\r\n",
        "    bar = '#' * filledLength + '-' * (barLength - filledLength)\r\n",
        "    sys.stdout.write('\\r%s |%s| %s%s %s' % (prefix, bar, percent, '%', suffix)),\r\n",
        "    if iteration == total:\r\n",
        "        sys.stdout.write('\\n')\r\n",
        "    sys.stdout.flush()"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49IAVxCp5UD3"
      },
      "source": [
        "json 데이터 가져오기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m86GuaxPxG8O"
      },
      "source": [
        "with open('friends_train.json') as json_file:\r\n",
        "    json_train = json.load(json_file)\r\n",
        "with open('friends_test.json') as json_file:\r\n",
        "    json_test = json.load(json_file)\r\n",
        "with open('friends_dev.json') as json_file:\r\n",
        "    json_dev = json.load(json_file)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rPNhfMpH5d9D"
      },
      "source": [
        "영어 데이터 전처리 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMukqKxNxP2l"
      },
      "source": [
        "def cleaning(str):\r\n",
        "    replaceAll = str\r\n",
        "    only_english = re.sub('[^a-zA-Z]', ' ', replaceAll)\r\n",
        "    no_capitals = only_english.lower().split()\r\n",
        "    no_stops = [word for word in no_capitals if not word in stops]\r\n",
        "    stemmer_words = [stemmer.stem(word) for word in no_stops]\r\n",
        "    return ' '.join(stemmer_words)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8G0C9X5t53pN"
      },
      "source": [
        "트레이닝용 데이터 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBI4YAXAxSr6"
      },
      "source": [
        "i = 0\r\n",
        "train_data=[]\r\n",
        "for rows in json_train:\r\n",
        "    for row in rows:\r\n",
        "        train_data.append([cleaning(row['utterance']), row['emotion']])\r\n",
        "for rows in json_test:\r\n",
        "    for row in rows:\r\n",
        "        train_data.append([cleaning(row['utterance']), row['emotion']])\r\n",
        "for rows in json_dev:\r\n",
        "    for row in rows:\r\n",
        "        train_data.append([cleaning(row['utterance']), row['emotion']])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "redqaFT26K_n"
      },
      "source": [
        "품사 태깅"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xl0MiK8_xYHx",
        "outputId": "5dcd20f9-3321-429b-e010-467f252a3614"
      },
      "source": [
        "cnt = 0\r\n",
        "tagged = []\r\n",
        "counter = collections.Counter()\r\n",
        "for d in train_data:\r\n",
        "    cnt = cnt + 1\r\n",
        "    printProgress(cnt, len(train_data), 'Progress:', 'Complete', 1, 100)\r\n",
        "    words = pos_tag(word_tokenize(d[0]))\r\n",
        "    for t in words:\r\n",
        "        word = \"/\".join(t)\r\n",
        "        tagged.append(word)\r\n",
        "        counter[word] += 1"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: |####################################################################################################| 100.0% Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P74wgezS6R4p"
      },
      "source": [
        "카운팅 끝난 후, 임베딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEHaM-HcxaR5"
      },
      "source": [
        "VOCAB_SIZE = 5000\r\n",
        "word2index = collections.defaultdict(int)\r\n",
        "for wid, word in enumerate(counter.most_common(VOCAB_SIZE)):\r\n",
        "    word2index[word[0]] = wid + 1\r\n",
        "vocab_sz = len(word2index) + 1\r\n",
        "index2word = {v:k for k, v in word2index.items()}"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rry5Fz06dT_"
      },
      "source": [
        "감정을 번호로, 번호를 감정으로 변환해주는 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HFx9GlU9xcpZ"
      },
      "source": [
        "def labeltoint(str):\r\n",
        "    return {'non-neutral': 0,\r\n",
        "             'neutral': 1, \r\n",
        "             'joy': 2,\r\n",
        "             'sadness': 3,\r\n",
        "             'fear': 4,\r\n",
        "             'anger': 5,\r\n",
        "             'surprise': 6,\r\n",
        "             'disgust': 7}[str]"
      ],
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UcHivnV06kHu"
      },
      "source": [
        "def inttolabel(idx):\r\n",
        "    return {0:'non-neutral',\r\n",
        "             1:'neutral', \r\n",
        "             2:'joy',\r\n",
        "             3:'sadness',\r\n",
        "             4:'fear',\r\n",
        "             5:'anger',\r\n",
        "             6:'surprise',\r\n",
        "             7:'disgust'}[idx]"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EP8CrEz66pzP"
      },
      "source": [
        "트레이닝 데이터의 emotion value를 모두 숫자로 변환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5S5WFd1xfKx",
        "outputId": "0213958b-80f5-45a3-dc5f-558040b1c5d3"
      },
      "source": [
        "xs, ys = [], []\r\n",
        "cnt = 0\r\n",
        "maxlen = 0\r\n",
        "for d in train_data:\r\n",
        "    cnt = cnt + 1\r\n",
        "    ys.append(labeltoint(d[1]))\r\n",
        "    printProgress(cnt, len(train_data), 'Progress:', 'Complete', 1, 100)\r\n",
        "    ang = pos_tag(word_tokenize(d[0]))\r\n",
        "    words=[]\r\n",
        "    for t in ang:\r\n",
        "        words.append(\"/\".join(t))\r\n",
        "    if len(words) > maxlen: \r\n",
        "        maxlen = len(words)\r\n",
        "    wids = [word2index[word] for word in words]\r\n",
        "    xs.append(wids)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: |####################################################################################################| 100.0% Complete\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J4Kg1qfY62tQ"
      },
      "source": [
        "X,Y 에 최종적인 train data와 train label 투입 후, 모델 생성"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXiFtmcV96uZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25ff365e-88a9-4c5d-9f12-022f0aad49b9"
      },
      "source": [
        "X = pad_sequences(xs, maxlen=maxlen) \r\n",
        "Y = np_utils.to_categorical(ys)\r\n",
        "\r\n",
        "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=50)\r\n",
        "\r\n",
        "model = Sequential() \r\n",
        "model.add(Embedding(vocab_sz, 100, input_length=maxlen))\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Conv1D(64, 5, padding='valid', activation=\"relu\", strides=1)) \r\n",
        "model.add(MaxPooling1D(pool_size=4))\r\n",
        "model.add(LSTM(100, activation='tanh')) \r\n",
        "model.add(Dense(8, activation=\"softmax\"))\r\n",
        "\r\n",
        "# es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=4)\r\n",
        "# mc = ModelCheckpoint('best_model.h5', monitor='accuracy', mode='max', verbose=1, save_best_only=True)\r\n",
        "\r\n",
        "model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"]) \r\n",
        "# history = model.fit(x_train, y_train, epochs=20, batch_size=64, callbacks=[mc], validation_data=(x_test, y_test))\r\n",
        "history = model.fit(x_train, y_train, epochs=20, batch_size=64, validation_data=(x_test, y_test))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "159/159 [==============================] - 15s 69ms/step - loss: 1.6873 - accuracy: 0.4414 - val_loss: 1.6064 - val_accuracy: 0.4500\n",
            "Epoch 2/20\n",
            "159/159 [==============================] - 10s 60ms/step - loss: 1.6011 - accuracy: 0.4432 - val_loss: 1.5896 - val_accuracy: 0.4498\n",
            "Epoch 3/20\n",
            "159/159 [==============================] - 10s 63ms/step - loss: 1.5661 - accuracy: 0.4601 - val_loss: 1.5831 - val_accuracy: 0.4484\n",
            "Epoch 4/20\n",
            "159/159 [==============================] - 10s 62ms/step - loss: 1.4685 - accuracy: 0.4968 - val_loss: 1.6104 - val_accuracy: 0.4415\n",
            "Epoch 5/20\n",
            "159/159 [==============================] - 10s 60ms/step - loss: 1.4224 - accuracy: 0.5062 - val_loss: 1.6646 - val_accuracy: 0.4346\n",
            "Epoch 6/20\n",
            "159/159 [==============================] - 10s 60ms/step - loss: 1.3586 - accuracy: 0.5313 - val_loss: 1.7235 - val_accuracy: 0.4259\n",
            "Epoch 7/20\n",
            "159/159 [==============================] - 10s 61ms/step - loss: 1.2784 - accuracy: 0.5568 - val_loss: 1.8047 - val_accuracy: 0.4261\n",
            "Epoch 8/20\n",
            "159/159 [==============================] - 10s 62ms/step - loss: 1.2457 - accuracy: 0.5714 - val_loss: 1.8823 - val_accuracy: 0.4250\n",
            "Epoch 9/20\n",
            "159/159 [==============================] - 9s 60ms/step - loss: 1.1811 - accuracy: 0.5911 - val_loss: 1.9268 - val_accuracy: 0.4252\n",
            "Epoch 10/20\n",
            "159/159 [==============================] - 9s 59ms/step - loss: 1.1450 - accuracy: 0.6105 - val_loss: 1.9845 - val_accuracy: 0.4321\n",
            "Epoch 11/20\n",
            "159/159 [==============================] - 6s 38ms/step - loss: 1.1017 - accuracy: 0.6183 - val_loss: 2.0403 - val_accuracy: 0.4234\n",
            "Epoch 12/20\n",
            "159/159 [==============================] - 6s 35ms/step - loss: 1.0984 - accuracy: 0.6198 - val_loss: 2.1020 - val_accuracy: 0.4171\n",
            "Epoch 13/20\n",
            "159/159 [==============================] - 6s 35ms/step - loss: 1.0673 - accuracy: 0.6339 - val_loss: 2.1852 - val_accuracy: 0.4192\n",
            "Epoch 14/20\n",
            "159/159 [==============================] - 6s 35ms/step - loss: 1.0577 - accuracy: 0.6410 - val_loss: 2.2232 - val_accuracy: 0.4121\n",
            "Epoch 15/20\n",
            "159/159 [==============================] - 6s 35ms/step - loss: 1.0206 - accuracy: 0.6458 - val_loss: 2.2281 - val_accuracy: 0.4142\n",
            "Epoch 16/20\n",
            "159/159 [==============================] - 6s 35ms/step - loss: 1.0264 - accuracy: 0.6438 - val_loss: 2.2707 - val_accuracy: 0.4165\n",
            "Epoch 17/20\n",
            "159/159 [==============================] - 6s 35ms/step - loss: 1.0044 - accuracy: 0.6542 - val_loss: 2.3289 - val_accuracy: 0.4194\n",
            "Epoch 18/20\n",
            "159/159 [==============================] - 6s 35ms/step - loss: 0.9780 - accuracy: 0.6606 - val_loss: 2.3807 - val_accuracy: 0.4206\n",
            "Epoch 19/20\n",
            "159/159 [==============================] - 6s 35ms/step - loss: 0.9999 - accuracy: 0.6470 - val_loss: 2.4064 - val_accuracy: 0.4135\n",
            "Epoch 20/20\n",
            "159/159 [==============================] - 6s 35ms/step - loss: 0.9886 - accuracy: 0.6581 - val_loss: 2.4022 - val_accuracy: 0.4123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bchxw5DtQaLi"
      },
      "source": [
        "가장 좋은 모델을 기준으로 테스트 데이터 정확도 측정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQJQWg98QZ2o",
        "outputId": "ea6382c3-a640-42a8-a5cc-cd6eb1d6df0a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# loaded_model = load_model('best_model.h5')\r\n",
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(x_test, y_test)[1]))"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "136/136 [==============================] - 1s 5ms/step - loss: 2.4022 - accuracy: 0.4123\n",
            "\n",
            " 테스트 정확도: 0.4123\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9YX6IIWRzu8"
      },
      "source": [
        "새로운 문장이 들어왔을 때 감정을 분석하는 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBjs46HJyts3"
      },
      "source": [
        "def sentiment_predict(text): \r\n",
        "    aa = pos_tag(word_tokenize(text))\r\n",
        "    pp = []\r\n",
        "    for t in aa:\r\n",
        "        pp.append(\"/\".join(t))\r\n",
        "    wids = [word2index[word] for word in pp]\r\n",
        "    x_predict = pad_sequences([wids], maxlen=maxlen) \r\n",
        "    # y_predict = loaded_model.predict(x_predict)\r\n",
        "    y_predict = model.predict(x_predict)\r\n",
        "    c = 0\r\n",
        "    cnt = 0\r\n",
        "    for y in y_predict[0]:\r\n",
        "        if c < y:\r\n",
        "            c = y\r\n",
        "            ans = cnt\r\n",
        "        cnt += 1\r\n",
        "    ans = inttolabel(ans)\r\n",
        "    return ans;"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wubxfjMRto4"
      },
      "source": [
        "Kaggle Sample 데이터를 통한 결과 분류 시작"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vOrwV77i0fGw",
        "outputId": "dbe64068-08e8-4be4-8711-acad78265b3e"
      },
      "source": [
        "predict_data = pd.read_csv('data.csv')\r\n",
        "total_length = len(predict_data)\r\n",
        "\r\n",
        "cols = ['Id', 'Predicted']\r\n",
        "lst = []\r\n",
        "for index, row in predict_data.iterrows():\r\n",
        "    printProgress(row['id'], total_length, 'Progress:', 'Complete', 1, 100)\r\n",
        "    lst.append([row['id'], sentiment_predict(row['utterance'])])\r\n",
        "\r\n",
        "result_df = pd.DataFrame(lst, columns=cols)\r\n",
        "print(result_df)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Progress: |####################################################################################################| 99.9% Complete        Id Predicted\n",
            "0        0   neutral\n",
            "1        1   neutral\n",
            "2        2     anger\n",
            "3        3   neutral\n",
            "4        4   neutral\n",
            "...    ...       ...\n",
            "1618  1618   neutral\n",
            "1619  1619   neutral\n",
            "1620  1620   neutral\n",
            "1621  1621   neutral\n",
            "1622  1622   neutral\n",
            "\n",
            "[1623 rows x 2 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c2s7tjNjRv_U"
      },
      "source": [
        "분석된 결과를 CSV 파일로 export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qIxrY3RMy8n-"
      },
      "source": [
        "result_df.to_csv('sample.csv', sep=',', index=False)"
      ],
      "execution_count": 49,
      "outputs": []
    }
  ]
}